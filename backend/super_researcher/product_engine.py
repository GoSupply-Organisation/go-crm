from decouple import config
import asyncio
import json
import weaviate
from weaviate.classes.config import Configure
from weaviate.util import generate_uuid5
from weaviate.classes.query import Sort 
from openai import AsyncOpenAI
from mcp import ClientSession, StdioServerParameters
from mcp.client.stdio import stdio_client
from prompting import reliability_prompt, urgency_prompt, search_system_prompt, question

# â”€â”€ YOUR INFERENCE ENGINE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# client = AsyncOpenAI(
#     base_url="http://127.0.0.1:8081/v1",
#     api_key="sk-no-key-required",
# )

# embed = AsyncOpenAI(
#     base_url="http://127.0.0.1:8082/v1",
#     api_key="sk-no-key-required",
# )

client = AsyncOpenAI(
    base_url=config("GLM_BASE_URL"),
    api_key=config("GLM_API_KEY"),
)

embed = AsyncOpenAI(
    api_key=config("OPENAI_API_KEY"),
)

# â”€â”€ MCP SERVER CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MCP_PARAMS = StdioServerParameters(
    command="uvx",
    args=["--python", ">=3.10,<3.14", "duckduckgo-mcp", "serve"],
)

# â”€â”€ EMBEDDING FUNCTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_embedding(text):
    try:
        response = embed.embeddings.create(
            input=text,
            model="text-embedding-3-large"
        )
        return response.data[0].embedding
    except Exception as e:
        print(f"Error generating embedding: {e}")
        return None

# â”€â”€ HELPER: CONVERT MCP TOOLS TO OPENAI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def get_openai_tools(session: ClientSession):
    """Fetches tools from the active MCP session and converts schema."""
    mcp_tools = await session.list_tools()
    return [
        {
            "type": "function",
            "function": {
                "name": tool.name,
                "description": tool.description,
                "parameters": tool.inputSchema,
            }
        }
        for tool in mcp_tools.tools
    ]

# â”€â”€ RELIABILITY AGENT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def reliability_agent(search_query: str, recent_context) -> dict:
    """
    Intakes search query -> searches web -> returns reliability rankings.
    """
    async with stdio_client(MCP_PARAMS) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()
            openai_tools = await get_openai_tools(session)
            messages = [
                {"role": "system", "content": f"{search_system_prompt}\n\n{reliability_prompt}\n\n{recent_context}"},
                {"role": "user", "content": search_query}
            ]

            while True:
                response = await client.chat.completions.create(
                    model="glm-4.7-flash",
                    tools=openai_tools,
                    messages=messages,
                    temperature=0.1,
                    max_tokens=2000,
                )

                message = response.choices[0].message
                
                if message.tool_calls:
                    messages.append(message)
                    for tool_call in message.tool_calls:
                        print(f"ğŸ”§ Calling Tool: {tool_call.function.name}")
                        result = await session.call_tool(
                            tool_call.function.name, 
                            arguments=json.loads(tool_call.function.arguments)
                        )
                        tool_content = result.content[0].text if result.content else ""
                        messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "content": tool_content
                        })
                    continue 
                else:
                    try:
                        return json.loads(message.content)
                    except json.JSONDecodeError:
                        return {"raw_response": message.content}

# â”€â”€ URGENCY AGENT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def urgency_agent(reliability_data: dict) -> list:
    """
    Intakes reliability data -> analyzes urgency via web search.
    """
    context = json.dumps(reliability_data, indent=2)

    async with stdio_client(MCP_PARAMS) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()
            openai_tools = await get_openai_tools(session)
            
            messages = [
                {"role": "system", "content": urgency_prompt},
                {"role": "user", "content": f"Analyze the urgency of these findings:\n\n{context}"}
            ]

            while True:
                response = await client.chat.completions.create(
                    model="glm-4.7-flash",
                    messages=messages,
                    temperature=0.1,
                    max_tokens=2000,
                    tools=openai_tools
                )

                message = response.choices[0].message

                if message.tool_calls:
                    messages.append(message)
                    for tool_call in message.tool_calls:
                        print(f"ğŸ”§ Calling Tool: {tool_call.function.name}")
                        result = await session.call_tool(
                            tool_call.function.name, 
                            arguments=json.loads(tool_call.function.arguments)
                        )
                        tool_content = result.content[0].text if result.content else ""
                        messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "content": tool_content
                        })
                    continue
                else:
                    try:
                        # Expecting a list of items
                        return json.loads(message.content)
                    except json.JSONDecodeError:
                        return [{"raw_response": message.content}]

def load_weivate_client():
    """
    Initializes and returns a Weaviate client connected to the local instance.
    """
    return weaviate.connect_to_local(
        host="localhost",
        port=8080,
        headers={
            "X-OpenAI-Api-Key": "sk-no-key-required"
        }
    )

def get_recent_context(limit: int = 5) -> str:
    """
    Fetches the most recent results from Weaviate and formats them 
    as a context string for an LLM prompt.
    """
    client = None
    try:
        # 1. Connect to local Weaviate
        client = load_weivate_client()
        
        collection = client.collections.get("Question")
        
        # 2. Query: Fetch objects sorted by 'date' (newest first)
        # NOTE: This requires your 'date' property to be in a valid date format.
        response = collection.query.fetch_objects(
            limit=limit,
            sort=Sort.by_property("date", descending=True),
            return_properties=["title", "url", "snippet", "date", "total_score", "urgency_score", "reliability_score"]
        )
        
        # 3. Format the results for the LLM
        if not response.objects:
            return "No recent data found in the database."

        # Build a clean text block
        context_string = "Here are the most recent findings from the database:\n\n"
        
        for i, obj in enumerate(response.objects, 1):
            props = obj.properties
            context_string += (
                f"--- Result {i} ---\n"
                f"Title: {props.get('title', 'N/A')}\n"
                f"Date: {props.get('date', 'N/A')}\n"
                f"Scores: Urgency {props.get('urgency_score', 0)} | Reliability {props.get('reliability_score', 0)} | Total {props.get('total_score', 0)}\n"
                f"Snippet: {props.get('snippet', 'N/A')}\n"
                f"URL: {props.get('url', 'N/A')}\n\n"
            )
            
        return context_string

    except Exception as e:
        return f"Error retrieving context from database: {e}"
    
    finally:
        if client:
            client.close()

# â”€â”€ WEAVIATE INTEGRATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_to_weaviate(reliability_output: dict, urgency_output: list):
    """
    Connects to Weaviate, sets up schema, and inserts the processed data.
    """
    print("\nğŸ’¾ Saving to Weaviate...")
    
    # 1. Connect to local Docker instance
    db_client = load_weivate_client()

    try:
        # 2. Define Collection Name
        collection_name = "Question"

        # 3. Clean slate (Optional: comment this out to keep existing data)
        if db_client.collections.exists(collection_name):
            db_client.collections.delete(collection_name)

        # 4. Create Collection with Vectorizer
        # Note: This points to your local embedding server via Docker internal URL
        questions = db_client.collections.create(
            name=collection_name,
            vector_config=Configure.Vectors.text2vec_openai(
                base_url="http://host.docker.internal:8082",
                model="qwen3-4b-embedding",
            ),
        )

        # 5. Prepare data map for easy lookup
        # Map URL -> Reliability Score
        reliability_map = {
            r_item['url']: r_item['score'] 
            for r_item in reliability_output.get('rankings', [])
        }

        # 6. Insert Data
        print(f"Loading {len(urgency_output)} items into Weaviate...")
        
        with questions.batch.dynamic() as batch:
            for item in urgency_output:
                url = item.get("url")
                u_score = item.get("urgency_score", 0)
                # Look up reliability score, default to 0 if not found
                r_score = reliability_map.get(url, 0)
                
                total_score = float(r_score) * float(u_score)

                # Generate deterministic UUID based on URL
                object_id = generate_uuid5(url)

                batch.add_object(
                    properties={
                        "title": item.get("title", "No Title"),
                        "url": url,
                        "snippet": item.get("snippet", ""),
                        "date": item.get("date"), 
                        "urgency_score": u_score,
                        "reliability_score": r_score,
                        "total_score": total_score,
                        "top_indicators": item.get("top_urgency_indicators", [])
                    },
                    uuid=object_id
                )
                
            if batch.number_errors > 10:
                print("Batch import stopped due to excessive errors.")
                failed_objects = questions.batch.failed_objects
                print(f"First failed object: {failed_objects[0]}")

        print(f"âœ… Successfully inserted {len(urgency_output)} objects.")

        # 7. Verify (Optional)
        response = questions.query.near_text(query="stock market", limit=2)
        print("\nğŸ” Verification Query Results:")
        for obj in response.objects:
            print(json.dumps(obj.properties, indent=2))

    except Exception as e:
        print(f"âŒ Weaviate Error: {e}")
    finally:
        db_client.close()

# â”€â”€ MAIN PIPELINE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def run_pipeline(search_query: str):
    """
    Full pipeline that:
    1. Starts with a search query
    2. Reliability agent searches and evaluates sources
    3. Urgency agent analyzes urgency based on findings
    4. Saves combined results to Weaviate
    """
    print("ğŸ” Initializing Pipeline...")

    print("Getting most recent context from Weaviate...")
    recent_context = get_recent_context()
    print(recent_context[:50] + "...\n")  # Print a snippet of the context


    print("\n" + "="*60)
    print("ğŸ“Š STAGE 1: Reliability Agent")
    print("="*60)
    reliability_output = await reliability_agent(search_query, recent_context)
    print(json.dumps(reliability_output, indent=2))

    print("\n" + "="*60)
    print("âš¡ STAGE 2: Urgency Agent")
    print("="*60)
    urgency_output = await urgency_agent(reliability_output)
    print(json.dumps(urgency_output, indent=2)) 

    # â”€â”€ SAVE TO WEAVIATE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if urgency_output:
        save_to_weaviate(reliability_output, urgency_output)
    else:
        print("No urgency data to save.")

    # Return final aggregated data
    return {
        "search_query": search_query,
        "reliability": reliability_output,
        "urgency": urgency_output
    }

# â”€â”€ RUN EXAMPLE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    search_query = question
    result = asyncio.run(run_pipeline(search_query))

    print("\n" + "="*60)
    print("ğŸ“‹ FINAL REPORT")
    print("="*60)
    print(json.dumps(result, indent=2))