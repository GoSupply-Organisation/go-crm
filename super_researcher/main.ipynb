{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.messages import HumanMessage, SystemMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.func import task\n",
    "from langgraph.func import entrypoint\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        model=\"local-lm\",\n",
    "        api_key=\"sk-xxx\",\n",
    "        base_url=\"http://127.0.0.1:1234/v1\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Schema for structured output to use in planning\n",
    "class Section(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"Name for this section of the report.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: List[Section] = Field(\n",
    "        description=\"Sections of the report.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "planner = llm.with_structured_output(Sections)\n",
    "\n",
    "\n",
    "@task\n",
    "def orchestrator(topic: str):\n",
    "    \"\"\"Orchestrator that generates a plan for the report\"\"\"\n",
    "    # Generate queries\n",
    "    report_sections = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Generate a plan for the report.\"),\n",
    "            HumanMessage(content=f\"Here is the report topic: {topic}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return report_sections.sections\n",
    "\n",
    "\n",
    "@task\n",
    "def llm_call(section: Section):\n",
    "    \"\"\"Worker writes a section of the report\"\"\"\n",
    "\n",
    "    # Generate section\n",
    "    result = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Write a report section.\"),\n",
    "            HumanMessage(\n",
    "                content=f\"Here is the section name: {section.name} and description: {section.description}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Write the updated section to completed sections\n",
    "    return result.content\n",
    "\n",
    "\n",
    "@task\n",
    "def synthesizer(completed_sections: list[str]):\n",
    "    \"\"\"Synthesize full report from sections\"\"\"\n",
    "    final_report = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
    "    return final_report\n",
    "\n",
    "\n",
    "@entrypoint()\n",
    "def orchestrator_worker(topic: str):\n",
    "    sections = orchestrator(topic).result()\n",
    "    section_futures = [llm_call(section) for section in sections]\n",
    "    final_report = synthesizer(\n",
    "        [section_fut.result() for section_fut in section_futures]\n",
    "    ).result()\n",
    "    return final_report\n",
    "\n",
    "# Invoke\n",
    "report = orchestrator_worker.invoke(\"Create a report on LLM scaling laws\")\n",
    "from IPython.display import Markdown\n",
    "Markdown(report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
